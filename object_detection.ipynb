{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8m.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/4/img_4.png\n",
      "\n",
      "image 1/1 /home/shawn/nvme/llmAgent_research/wli/LMD/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/4/img_4.png: 640x640 9 kites, 1 surfboard, 6.9ms\n",
      "Speed: 4.2ms preprocess, 6.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mobject_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_42\u001b[0m\n",
      "1 label saved to object_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_42/labels\n",
      "img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/10/img_4.png\n",
      "\n",
      "image 1/1 /home/shawn/nvme/llmAgent_research/wli/LMD/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/10/img_4.png: 640x640 (no detections), 6.2ms\n",
      "Speed: 1.6ms preprocess, 6.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mobject_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_102\u001b[0m\n",
      "0 label saved to object_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_102/labels\n",
      "img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/7/img_4.png\n",
      "\n",
      "image 1/1 /home/shawn/nvme/llmAgent_research/wli/LMD/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/7/img_4.png: 640x640 5 persons, 1 bottle, 4 cups, 6 chairs, 2 dining tables, 6.1ms\n",
      "Speed: 1.7ms preprocess, 6.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mobject_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_72\u001b[0m\n",
      "1 label saved to object_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_72/labels\n",
      "img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/2/img_4.png\n",
      "\n",
      "image 1/1 /home/shawn/nvme/llmAgent_research/wli/LMD/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/2/img_4.png: 640x640 7 bicycles, 6.0ms\n",
      "Speed: 1.6ms preprocess, 6.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mobject_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_22\u001b[0m\n",
      "1 label saved to object_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_22/labels\n",
      "img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/14/img_4.png\n",
      "\n",
      "image 1/1 /home/shawn/nvme/llmAgent_research/wli/LMD/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/14/img_4.png: 640x640 (no detections), 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mobject_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_142\u001b[0m\n",
      "0 label saved to object_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_142/labels\n",
      "img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/9/img_4.png\n",
      "\n",
      "image 1/1 /home/shawn/nvme/llmAgent_research/wli/LMD/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/9/img_4.png: 640x640 1 person, 1 truck, 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mobject_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_92\u001b[0m\n",
      "1 label saved to object_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_92/labels\n",
      "img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/13/img_4.png\n",
      "\n",
      "image 1/1 /home/shawn/nvme/llmAgent_research/wli/LMD/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/13/img_4.png: 640x640 (no detections), 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mobject_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_132\u001b[0m\n",
      "0 label saved to object_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_132/labels\n",
      "img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/15/img_4.png\n",
      "\n",
      "image 1/1 /home/shawn/nvme/llmAgent_research/wli/LMD/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/15/img_4.png: 640x640 (no detections), 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mobject_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_152\u001b[0m\n",
      "0 label saved to object_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_152/labels\n",
      "img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/8/img_4.png\n",
      "\n",
      "image 1/1 /home/shawn/nvme/llmAgent_research/wli/LMD/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/8/img_4.png: 640x640 1 person, 1 potted plant, 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mobject_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_82\u001b[0m\n",
      "1 label saved to object_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_82/labels\n",
      "img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/16/img_4.png\n",
      "\n",
      "image 1/1 /home/shawn/nvme/llmAgent_research/wli/LMD/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/16/img_4.png: 640x640 2 trains, 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mobject_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_162\u001b[0m\n",
      "1 label saved to object_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_162/labels\n",
      "img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/11/img_4.png\n",
      "\n",
      "image 1/1 /home/shawn/nvme/llmAgent_research/wli/LMD/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/11/img_4.png: 640x640 2 birds, 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mobject_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_112\u001b[0m\n",
      "1 label saved to object_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_112/labels\n",
      "img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/1/img_4.png\n",
      "\n",
      "image 1/1 /home/shawn/nvme/llmAgent_research/wli/LMD/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/1/img_4.png: 640x640 2 cats, 1 suitcase, 1 couch, 6.0ms\n",
      "Speed: 1.4ms preprocess, 6.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mobject_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_110\u001b[0m\n",
      "1 label saved to object_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_110/labels\n",
      "img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/18/img_4.png\n",
      "\n",
      "image 1/1 /home/shawn/nvme/llmAgent_research/wli/LMD/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/18/img_4.png: 640x640 1 mouse, 6.4ms\n",
      "Speed: 1.4ms preprocess, 6.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mobject_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_182\u001b[0m\n",
      "1 label saved to object_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_182/labels\n",
      "img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/19/img_4.png\n",
      "\n",
      "image 1/1 /home/shawn/nvme/llmAgent_research/wli/LMD/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/19/img_4.png: 640x640 2 chairs, 4 refrigerators, 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mobject_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_192\u001b[0m\n",
      "1 label saved to object_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_192/labels\n",
      "img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/3/img_4.png\n",
      "\n",
      "image 1/1 /home/shawn/nvme/llmAgent_research/wli/LMD/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/3/img_4.png: 640x640 2 boats, 2 birds, 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mobject_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_32\u001b[0m\n",
      "1 label saved to object_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_32/labels\n",
      "img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/17/img_4.png\n",
      "\n",
      "image 1/1 /home/shawn/nvme/llmAgent_research/wli/LMD/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/17/img_4.png: 640x640 5 horses, 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mobject_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_172\u001b[0m\n",
      "1 label saved to object_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_172/labels\n",
      "img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/12/img_4.png\n",
      "\n",
      "image 1/1 /home/shawn/nvme/llmAgent_research/wli/LMD/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/12/img_4.png: 640x640 (no detections), 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mobject_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_122\u001b[0m\n",
      "0 label saved to object_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_122/labels\n",
      "img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/6/img_4.png\n",
      "\n",
      "image 1/1 /home/shawn/nvme/llmAgent_research/wli/LMD/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/6/img_4.png: 640x640 (no detections), 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mobject_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_62\u001b[0m\n",
      "0 label saved to object_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_62/labels\n",
      "img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/5/img_4.png\n",
      "\n",
      "image 1/1 /home/shawn/nvme/llmAgent_research/wli/LMD/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/5/img_4.png: 640x640 (no detections), 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mobject_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_52\u001b[0m\n",
      "0 label saved to object_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_52/labels\n",
      "img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/0/img_4.png\n",
      "\n",
      "image 1/1 /home/shawn/nvme/llmAgent_research/wli/LMD/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/0/img_4.png: 640x640 4 persons, 3 umbrellas, 1 handbag, 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mobject_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_02\u001b[0m\n",
      "1 label saved to object_detection/img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0/results_02/labels\n"
     ]
    }
   ],
   "source": [
    "# directory to detect objects in\n",
    "\n",
    "DIR = \"img_generations/img_generations_templatev0.1_lmd_plus_demo_gpt-4/run0\"\n",
    "for dir in os.listdir(DIR):\n",
    "    if os.path.isdir(os.path.join(DIR, dir)):\n",
    "        for file in os.listdir(os.path.join(DIR, dir)):\n",
    "            if file.endswith(\"4.png\"):\n",
    "                cur_path = os.path.join(DIR, dir, file)\n",
    "                print(cur_path)\n",
    "                results = model.predict(os.path.join(DIR, dir, file), save=True, save_txt=True, project=f\"object_detection/{DIR}\", name=f\"results_{dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_gd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
